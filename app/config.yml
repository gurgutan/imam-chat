# Config file for imam-chat application

# Data loader settings.
# Implemented providers [15.03.24]: WebBaseLoader, JsonLoader, TextLoader
loader:
  # provider: WebBaseLoader
  # uri:  https://docs.haqq.network/learn/glossary/

  provider: JsonLoader
  uri: data/quran_dict.json
  jq_schema: .data[].tafsir_en

  # provider: TextLoader
  # uri: data/quran.txt
  # encoding: utf-8

# Embeddings settings.
# Implemented providers [15.03.24]: GPT4AllEmbeddings, HuggingFaceEmbeddings
embedder: 
  # provider: GPT4AllEmbeddings
  # model_name: 
  
  provider: HuggingFaceEmbeddings
  model_name: sentence-transformers/all-mpnet-base-v2

# Vector database settings
# Implemented providers [15.03.24]: chromadb
vectordb:
  provider: chroma
  path: ./db
  search_kwargs:
    k: 4
  anonymized_telemetry: false
  # search_type: similarity_score_threshold 
  # score_threshold: 0.1

prompt_template: |
  "Answer the question based only on the following context:
  {context}

  Question: {question}

  Answer:"

# LLM settings. Implemented providers [15.03.24]: llamacpp, ctransformers
llm:
  # Implemented providers: llamacpp, ctransformers
  # provider: llamacpp
  # Parameter 'model' is path to model directory
  # model: models/saiga-mistral-7b
  # 'model_file' is file name, so full path to model file is 'model/model_file'
  # model_file: saiga-mistral-q4_K.gguf

  # if model name has format Author/model_name, it will try download model hosted on the Hugging Face Hub
  provider: ctransformers
  model: TheBloke/Llama-2-7B-Chat-GGUF
  model_file: llama-2-7b-chat.Q4_0.gguf

  # provider: openai
  # model: TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q6_K.gguf # gpt-3.5-turbo-0125
  # base_url: http://localhost:8001/v1 # https://api.openai.com/v1
  # api_key: not_needed

  temperature: 0.1
  max_tokens: 2048
  top_k: 1
  top_p: 0.95
  n_ctx: 4096
  model_type: llama
  threads: 14

  

# Server settings
server:
  host: 127.0.0.1
  port: 8010
  auth: false
