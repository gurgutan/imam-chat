# Config file for imam-chat application

# Data loader settings.
# Implemented providers [15.03.24]: WebBaseLoader, JsonLoader, TextLoader
loader:
  # provider: WebBaseLoader
  # uri:  https://docs.haqq.network/learn/glossary/

  provider: JsonLoader
  uri: data/quran_dict.json
  jq_schema: .data[].tafsir_en

  # provider: TextLoader
  # uri: data/quran.txt
  # encoding: utf-8

# Embeddings settings.
# Implemented providers [15.03.24]: GPT4AllEmbeddings, HuggingFaceEmbeddings
embedder: 
  # provider: GPT4AllEmbeddings
  # model_name: 
  
  provider: HuggingFaceEmbeddings
  model_name: BAAI/bge-small-en-v1.5 #mixedbread-ai/mxbai-embed-large-v1 #sentence-transformers/all-mpnet-base-v2 #

  # provider: NomicEmbeddings
  # model_name: nomic-ai/nomic-embed-text-v1.5

# Vector database settings
# Implemented providers [15.03.24]: chromadb
vectordb:
  provider: chroma
  path: ./db
  search_kwargs:
    k: 2
  anonymized_telemetry: false
  rebuild: false
  # search_type: similarity_score_threshold 
  # score_threshold: 0.1

prompt_template: |
  "Answer the question based only on the following context:
  {context}

  Question: {question}

  Answer:"

# LLM settings. Implemented providers [15.03.24]: llamacpp, ctransformers
llm:
  # Implemented providers: llamacpp, ctransformers
  # provider: llamacpp
  # # Parameter 'model' is path to model directory
  # model: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  # # 'model_file' is file name, so full path to model file is 'model/model_file'
  # model_file: mistral-7b-instruct-v0.2.Q6_K.gguf

  # if model name has format Author/model_name, it will try download model hosted on the Hugging Face Hub
  provider: ctransformers
  model: TheBloke/Llama-2-7B-Chat-GGUF
  model_file: llama-2-7b-chat.Q4_0.gguf

  # provider: openai
  # model: TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q6_K.gguf # gpt-3.5-turbo-0125
  # base_url: https://api.proxyapi.ru/openai/v1 # http://localhost:8001/v1 # https://api.openai.com/v1
  # api_key: sk-kwrRdqybjppKSpSp60ZBbztAAdqtgpRCs

  temperature: 0.1
  max_tokens: 2048
  top_k: 1
  top_p: 0.95
  n_ctx: 4096
  model_type: llama
  threads: 14

  

# Server settings
server:
  host: 127.0.0.1
  port: 8010
  auth: false
