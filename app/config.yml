# Config file for imam-chat application

# Data loader settings.
# Implemented providers [15.03.24]: WebBaseLoader, JsonLoader, TextLoader
loader:
  provider: WebBaseLoader
  uri: https://ummah.su/info/terminy-islama

  # provider: JsonLoader
  # uri: data/quran_dict.json
  # jq_schema: .data[].tafsir_ru

  # provider: TextLoader
  # uri: data/quran.txt
  # encoding: utf-8

# Embeddings settings.
# Implemented providers [15.03.24]: GPT4AllEmbeddings, HuggingFaceEmbeddings
embedder: 
  provider: GPT4AllEmbeddings
  # model_name: 
  
  # provider: HuggingFaceEmbeddings
  # model_name: sentence-transformers/all-mpnet-base-v2

# Vector database settings
# Implemented providers [15.03.24]: chromadb
vectordb:
  provider: chroma
  path: ./db
  search_kwargs:
    k: 1
  anonymized_telemetry: false
  # search_type: similarity_score_threshold 
  # score_threshold: 0.1

prompt_template: |
  "Answer the question based only on the following context:
  {context}

  Question: {question}
  "

# LLM settings. Implemented providers [15.03.24]: llamacpp, ctransformers
llm:
  # Implemented providers: llamacpp, ctransformers
  # provider: llamacpp
  # Parameter 'model' is path to model directory
  # model: models/saiga-mistral-7b
  # 'model_file' is file name, so full path to model file is 'model/model_file'
  # model_file: saiga-mistral-q4_K.gguf

  # if model name has format Author/model_name, it will try download model hosted on the Hugging Face Hub
  provider: ctransformers
  model: TheBloke/Llama-2-7B-Chat-GGUF
  model_file: llama-2-7b-chat.Q4_0.gguf
  temperature: 0.1
  max_tokens: 2048
  top_k: 1
  top_p: 0.95
  n_ctx: 4096
  model_type: llama
  threads: 6
  # openai_base_url: http://localhost:8004/v1
  # openai_api_key: not_needed


# Server settings
server:
  host: 127.0.0.1
  port: 8010
  auth: false
