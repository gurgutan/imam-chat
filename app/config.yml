# Config file for imam-chat application


# Data loader settings.
# Implemented providers [11.04.24]: WebBaseLoader, JsonLoader, TextLoader, QuranJsonLoader
loader:
  # provider: WebBaseLoader
  # uri:  https://docs.haqq.network/learn/glossary/

  # provider: JsonLoader
  # uri: sources/quran_en.json
  # jq_schema: .data[]
  # content_key: .tafsir

  provider: QuranJsonLoader
  uri: sources/quran-en.json

  # provider: TextLoader
  # uri: data/quran.txt
  # encoding: utf-8


# Embeddings settings.
# Implemented providers [07.04.24]: GPT4AllEmbeddings, HuggingFaceEmbeddings, huggingfacebgeembedding
embedder: 
  # provider: GPT4AllEmbeddings
  # model_name: 
  provider: huggingfacebgeembedding
  model_name: BAAI/bge-small-en-v1.5
  
  # provider: HuggingFaceEmbeddings
  # model_name: sentence-transformers/all-mpnet-base-v2 #mixedbread-ai/mxbai-embed-large-v1 #

  # provider: openaiembeddings
  # model_name:

  model_kwargs:
    device: cpu # cuda, cpu


# Vector database settings
# Implemented providers [07.04.24]: chromadb, faiss
vectordb:
  provider: faiss
  metric_type: cosine # l2, ip, jaccard
  search_kwargs:
    k: 4
  anonymized_telemetry: false
  # search_type: similarity_score_threshold 
  # score_threshold: 0.1

# Not implemented yet
# dialogsdb:
#   user: 
#   password:
#   port:
#   host:


# Using of prompt template not implemented yet (07.04.2024)
prompt_template: |
  "Answer the question based only on the following context:
  {context}

  Question: {question}

  Answer:"


# LLM settings. Implemented providers [07.04.24]: llamacpp, ctransformers, vllmopenai, openai
llm:
  # Implemented providers: llamacpp, ctransformers
  provider: llamacpp
  # Parameter 'model' is path to model directory
  model: ./models
  # 'model_file' is file name, so full path to model file is 'model/model_file'
  model_file: mistral-7b-instruct-v0.2.Q4_0.gguf

  # if model name has format Author/model_name, it will try download model hosted on the Hugging Face Hub
  # provider: ctransformers
  # model: TheBloke/Llama-2-7B-Chat-GGUF
  # model_file: llama-2-7b-chat.Q4_0.gguf

  # provider: vllmopenai
  # model: gpt-3.5-turbo 
  # base_url: http://localhost:8001/v1
  # api_key: not_need

  # provider: openai
  # model: gpt-3.5-turbo
  # base_url: https://api.openai.com/v1
  # api_key: sk-some_key


  temperature: 0.1
  max_tokens: 1024 # maximum number of tokens in generated text
  top_k: 1
  top_p: 0.95
  n_ctx: 4096
  model_type: llama
  threads: 32 # number of CPU threads to use
 

# Server settings
server:
  host: 127.0.0.1
  port: 8010
  auth: false
